---
layout: post
date: 2023-09-06
inline: true
---

2 papers accepted to [IJCNLP-AACL 2023](http://www.ijcnlp-aacl2023.org/)!

***
Our paper entitled [Benchmarking Procedural Language Understanding for Low-Resource Languages: A Case Study on Turkish]({{ '/assets/pdf/2309.11346.pdf' | relative_url }}) is accepted to the main [IJCNLP-AACL 2023](http://www.ijcnlp-aacl2023.org/) conference! Check the [repo](https://github.com/GGLAB-KU/turkish-plu) for more details ðŸ“£

> Abstract: Understanding procedural natural language (e.g., step-by-step instructions) is a crucial step to execution and planning. However, while there are ample corpora and downstream tasks available in English, the field lacks such resources for most languages. To address this gap, we conduct a case study on Turkish procedural texts. We first expand the number of tutorials in Turkish wikiHow from 2,000 to 52,000 using automated translation tools, where the translation quality and loyalty to the original meaning are validated by a team of experts on a random set. Then, we generate several downstream tasks on the corpus, such as linking actions, goal inference, and summarization. To tackle these tasks, we implement strong baseline models via fine-tuning large language-specific models such as TR-BART and BERTurk, as well as multilingual models such as mBART, mT5, and XLM. We find that language-specific models consistently outperform their multilingual models by a significant margin across most procedural language understanding~(PLU) tasks. 

***
Another paper [GECTurk: Grammatical Error Correction and Detection Dataset for Turkish]({{ '/assets/pdf/2309.11346.pdf' | relative_url }}) is accepted to the [Findings of IJCNLP-AACL 2023](http://www.ijcnlp-aacl2023.org/) ðŸ“£ 

> Abstract: Grammatical Error Detection and Correction (GEC) tools have proven useful for native speakers and second language learners. Developing such tools requires a large amount of parallel, annotated data, which is unavailable for most languages. Synthetic data generation is a common practice to overcome the scarcity of such data. However, it is not straightforward for morphologically rich languages like Turkish due to complex writing rules that require phonological, morphological, and syntactic information. In this work, we present a flexible and extensible synthetic data generation pipeline for Turkish covering more than 20 expert-curated grammar and spelling rules (a.k.a., writing rules) implemented through complex transformation functions. Using the pipeline, we derive 130,000 high-quality parallel sentences from professionally edited articles. Additionally, we create a more realistic test set by manually annotating a set of movie reviews. We implement three baselines formulating the task as i) neural machine translation, ii) sequence tagging, and iii) few-shot learning with prefix tuning, achieving strong results. Then we perform a zero-shot evaluation of our pretrained models on the coarse-grained "BOUN -de/-da" and fine-grained expert annotated dataset. Our results suggest that our corpus, GECTurk, is high-quality and allows knowledge transfer for the out-of-domain setting. To encourage further research on Turkish GEC, we release our dataset, baseline models, and synthetic data generation pipeline with https://anonymous.4open.science/r/tr-gec-17D6/.