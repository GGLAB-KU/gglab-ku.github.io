<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Publications | GGLab  </title>
    <meta name="author" content="GGLab  ">
    <meta name="description" content="Publications by categories in reversed chronological order.">
    <meta name="keywords" content="natural language processing, NLP, AI, Language Models, LLM">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon144x144.png">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://gglab-ku.github.io/publications">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header --><header class="fixed-top">

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand">
    <div class="container px-md-3">
      <a href="/" class="">
        <img src="/assets/img/gglab-logo-light.png" alt="GGLab   " class="logo repo-img-light">
        <img src="/assets/img/gglab-logo-dark.png" alt="GGLab   " class="logo repo-img-dark">
      </a>
      
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-wrap">
          <!-- About -->
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link text-nowrap" href="/">
                Home
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link text-nowrap" href="/team">
                Team
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link text-nowrap" href="/projects/">
                Projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item active">
              <a class="nav-link text-nowrap" href="/publications">
                Publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link text-nowrap" href="/open-positions">
                Open Positions
                
              </a>
          </li>
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">Publications</h1>
            <p class="post-description">Publications by categories in reversed chronological order.</p>
          </header>

          <article>
            <!-- _pages/publications.md -->
<div class="publications">
  <h2 class="year">2025</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#7a9b76"><a href="https://naacl.org/" rel="external nofollow noopener" target="_blank">NAACL</a></abbr></div>

        <!-- Entry bib key -->
        <div id="safa2024zeroshotopenvocabularypipelinedialogue" class="col-sm-8">
        <!-- Title -->
        <div class="title">A Zero-Shot Open-Vocabulary Pipeline for Dialogue Understanding</div>
        <!-- Author -->
        <div class="author">
        

        Abdulfattah Safa, and Gözde Gül Şahin</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the 2025 Annual Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics</em>, Apr 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/2409.15861v1.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Dialogue State Tracking (DST) is crucial for understanding user needs and executing appropriate system actions in task-oriented dialogues. Majority of existing DST methods are designed to work within predefined ontologies and assume the availability of gold domain labels, struggling with adapting to new slots values. While Large Language Models (LLMs)-based systems show promising zero-shot DST performance, they either require extensive computational resources or they underperform existing fully-trained systems, limiting their practicality. To address these limitations, we propose a zero-shot, open-vocabulary system that integrates domain classification and DST in a single pipeline. Our approach includes reformulating DST as a question-answering task for less capable models and employing self-refining prompts for more adaptable ones. Our system does not rely on fixed slot values defined in the ontology allowing the system to adapt dynamically. We compare our approach with existing SOTA, and show that it provides up to 20% better Joint Goal Accuracy (JGA) over previous methods on datasets like Multi-WOZ 2.1, with up to 90% fewer requests to the LLM API.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">safa2024zeroshotopenvocabularypipelinedialogue</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Zero-Shot Open-Vocabulary Pipeline for Dialogue Understanding}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Safa, Abdulfattah and Şahin, Gözde Gül}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2025 Annual Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New Mexico, USA}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{00-00}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#ff9900"><a href="https://aclanthology.org/venues/coling/" rel="external nofollow noopener" target="_blank">COLING</a></abbr></div>

        <!-- Entry bib key -->
        <div id="gebeşçe2024gecturkwebexplainableonline" class="col-sm-8">
        <!-- Title -->
        <div class="title">GECTurk WEB: An Explainable Online Platform for Turkish Grammatical Error Detection and Correction</div>
        <!-- Author -->
        <div class="author">
        

        Ali Gebeşçe, and Gözde Gül Şahin</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations</em>, Jan 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/2410.12350v1.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Sophisticated grammatical error detection/correction tools are available for a small set of languages such as English and Chinese. However, it is not straightforward – if not impossible – to adapt them to morphologically rich languages with complex writing rules like Turkish which has more than 80 million speakers. Even though several tools exist for Turkish, they primarily focus on spelling errors rather than grammatical errors and lack features such as web interfaces, error explanations and feedback mechanisms. To fill this gap, we introduce GECTurk WEB, a light, open-source, and flexible web-based system that can detect and correct the most common forms of Turkish writing errors, such as the misuse of diacritics, compound and foreign words, pronouns, light verbs along with spelling mistakes. Our system provides native speakers and second language learners an easily accessible tool to detect/correct such mistakes and also to learn from their mistakes by showing the explanation for the violated rule(s). The proposed system achieves 88,3 system usability score, and is shown to help learn/remember a grammatical rule (confirmed by 80% of the participants).</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gebeşçe2024gecturkwebexplainableonline</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{GECTurk WEB: An Explainable Online Platform for Turkish Grammatical Error Detection and Correction}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gebeşçe, Ali and Şahin, Gözde Gül}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">adress</span> <span class="p">=</span> <span class="s">{Abu Dhabi, UAE}</span><span class="p">,</span>
  <span class="na">pulisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2025.coling-demos.16/}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{163--173}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2024</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#8b4513"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a></abbr></div>

        <!-- Entry bib key -->
        <div id="safa2024systematicsurveyinstructionaltext" class="col-sm-8">
        <!-- Title -->
        <div class="title">A Systematic Survey on Instructional Text: From Representation Formats to Downstream NLP Tasks</div>
        <!-- Author -->
        <div class="author">
        

        Abdulfattah Safa, Tamta Kapanadze, Arda Uzunoğlu, and
          <span class="more-authors" title="click to view 1 more author" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '1 more author' ? 'Gözde Gül Şahin' : '1 more author';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">1 more author</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          Jan 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/2410.18529v2.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-arxiv-id="2410.18529"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Recent advances in large language models have demonstrated promising capabilities in following simple instructions through instruction tuning. However, real-world tasks often involve complex, multi-step instructions that remain challenging for current NLP systems. Despite growing interest in this area, there lacks a comprehensive survey that systematically analyzes the landscape of complex instruction understanding and processing. Through a systematic review of the literature, we analyze available resources, representation schemes, and downstream tasks related to instructional text. Our study examines 177 papers, identifying trends, challenges, and opportunities in this emerging field. We provide AI/NLP researchers with essential background knowledge and a unified view of various approaches to complex instruction understanding, bridging gaps between different research directions and highlighting future research opportunities.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">safa2024systematicsurveyinstructionaltext</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Systematic Survey on Instructional Text: From Representation Formats to Downstream NLP Tasks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Safa, Abdulfattah and Kapanadze, Tamta and Uzunoğlu, Arda and Şahin, Gözde Gül}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2410.18529}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#8b4513"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a></abbr></div>

        <!-- Entry bib key -->
        <div id="soykan2024linguisticallyinformedmultilingualinstructiontuning" class="col-sm-8">
        <!-- Title -->
        <div class="title">Linguistically-Informed Multilingual Instruction Tuning: Is There an Optimal Set of Languages to Tune?</div>
        <!-- Author -->
        <div class="author">
        

        Gürkan Soykan, and Gözde Gül Şahin</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          Jan 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/2410.07809v1.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-arxiv-id="2410.07809"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Multilingual language models often perform unevenly across different languages due to limited generalization capabilities for some languages. This issue is significant because of the growing interest in making universal language models that work well for all languages. Instruction tuning with multilingual instruction-response pairs has been used to improve model performance across various languages. However, this approach is challenged by high computational costs, a lack of quality tuning data for all languages, and the "curse of multilinguality" – the performance drop per language after adding many languages. Recent studies have found that working with datasets with few languages and a smaller number of instances can be beneficial. Yet, there exists no systematic investigation into how choosing different languages affects multilingual instruction tuning. Our study proposes a method to select languages for instruction tuning in a linguistically informed way, aiming to boost model performance across languages and tasks. We use a simple algorithm to choose diverse languages and test their effectiveness on various benchmarks and open-ended questions. Our results show that this careful selection generally leads to better outcomes than choosing languages at random. We suggest a new and simple way of enhancing multilingual models by selecting diverse languages based on linguistic features that could help develop better multilingual systems and guide dataset creation efforts.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">soykan2024linguisticallyinformedmultilingualinstructiontuning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Linguistically-Informed Multilingual Instruction Tuning: Is There an Optimal Set of Languages to Tune?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Soykan, Gürkan and Şahin, Gözde Gül}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2410.07809}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#8b4513"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a></abbr></div>

        <!-- Entry bib key -->
        <div id="uzunoglu2024paradise" class="col-sm-8">
        <!-- Title -->
        <div class="title">PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset</div>
        <!-- Author -->
        <div class="author">
        

        Arda Uzunoglu, Abdulfattah Safa, and Gözde Gül Şahin</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          Mar 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/2403.03167.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-arxiv-id="2403.03167"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Recently, there has been growing interest within the community regarding whether large language models are capable of planning or executing plans. However, most prior studies use LLMs to generate high-level plans for simplified scenarios lacking linguistic complexity and domain diversity, limiting analysis of their planning abilities. These setups constrain evaluation methods (e.g., predefined action space), architectural choices (e.g., only generative models), and overlook the linguistic nuances essential for realistic analysis. To tackle this, we present PARADISE, an abductive reasoning task using Q&amp;A format on practical procedural text sourced from wikiHow. It involves warning and tip inference tasks directly associated with goals, excluding intermediary steps, with the aim of testing the ability of the models to infer implicit knowledge of the plan solely from the given goal. Our experiments, utilizing fine-tuned language models and zero-shot prompting, reveal the effectiveness of task-specific small models over large language models in most scenarios. Despite advancements, all models fall short of human performance. Notably, our analysis uncovers intriguing insights, such as variations in model behavior with dropped keywords, struggles of BERT-family and GPT-4 with physical and abstract goals, and the proposed tasks offering valuable prior knowledge for other unseen procedural tasks. The PARADISE dataset and associated resources are publicly available for further research exploration with this https URL.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">uzunoglu2024paradise</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Uzunoglu, Arda and Safa, Abdulfattah and Şahin, Gözde Gül}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2403.03167}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2023</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#8b4513"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a></abbr></div>

        <!-- Entry bib key -->
        <div id="kural2024quantifying" class="col-sm-8">
        <!-- Title -->
        <div class="title">Quantifying Divergence for Human-AI Collaboration and Cognitive Trust</div>
        <!-- Author -->
        <div class="author">
        

        Müge Kural, Ali Gebeşçe, Tilek Chubakov, and
          <span class="more-authors" title="click to view 1 more author" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '1 more author' ? 'Gözde Gül Şahin' : '1 more author';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">1 more author</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          Dec 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/2312.08722.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-arxiv-id="2312.08722"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Predicting the collaboration likelihood and measuring cognitive trust to AI systems is more important than ever. To do that, previous research mostly focus solely on the model features (e.g., accuracy, confidence) and ignore the human factor. To address that, we propose several decision-making similarity measures based on divergence metrics (e.g., KL, JSD) calculated over the labels acquired from humans and a wide range of models. We conduct a user study on a textual entailment task, where the users are provided with soft labels from various models and asked to pick the closest option to them. The users are then shown the similarities/differences to their most similar model and are surveyed for their likelihood of collaboration and cognitive trust to the selected system. Finally, we qualitatively and quantitatively analyze the relation between the proposed decision-making similarity measures and the survey results. We find that people tend to collaborate with their most similar models – measured via JSD – yet this collaboration does not necessarily imply a similar level of cognitive trust. We release all resources related to the user study (e.g., design, outputs), models, and metrics at our repo.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">kural2024quantifying</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Quantifying Divergence for Human-AI Collaboration and Cognitive Trust}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kural, Müge and Gebeşçe, Ali and Chubakov, Tilek and Şahin, Gözde Gül}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2312.08722}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.AI}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c27ba0"><a href="https://aclanthology.org/venues/ijcnlp/" rel="external nofollow noopener" target="_blank">IJCNLP-AACL</a></abbr></div>

        <!-- Entry bib key -->
        <div id="uzunoglu-ahin:2023:ijcnlp" class="col-sm-8">
        <!-- Title -->
        <div class="title">Benchmarking Procedural Language Understanding for Low-Resource Languages: A Case Study on Turkish</div>
        <!-- Author -->
        <div class="author">
        

        Arda Uzunoglu, and Gözde Şahin</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics</em>, Nov 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/2309.06698.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Understanding procedural natural language (e.g., step-by-step instructions) is a crucial step to execution and planning. However, while there are ample corpora and downstream tasks available in English, the field lacks such resources for most languages. To address this gap, we conduct a case study on Turkish procedural texts. We first expand the number of tutorials in Turkish wikiHow from 2,000 to 52,000 using automated translation tools, where the translation quality and loyalty to the original meaning are validated by a team of experts on a random set. Then, we generate several downstream tasks on the corpus, such as linking actions, goal inference, and summarization. To tackle these tasks, we implement strong baseline models via fine-tuning large language-specific models such as TR-BART and BERTurk, as well as multilingual models such as mBART, mT5, and XLM. We find that language-specific models consistently outperform their multilingual models by a significant margin across most procedural language understanding (PLU) tasks.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">uzunoglu-ahin:2023:ijcnlp</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Uzunoglu, Arda and Şahin, Gözde}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Benchmarking Procedural Language Understanding for Low-Resource Languages: A Case Study on Turkish}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Nusa Dua, Bali}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{804--819}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.ijcnlp-long.52}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c27ba0"><a href="https://aclanthology.org/venues/ijcnlp/" rel="external nofollow noopener" target="_blank">IJCNLP-AACL</a></abbr></div>

        <!-- Entry bib key -->
        <div id="kara-EtAl:2023:findings" class="col-sm-8">
        <!-- Title -->
        <div class="title">GECTurk: Grammatical Error Correction and Detection Dataset for Turkish</div>
        <!-- Author -->
        <div class="author">
        

        Atakan Kara, Farrin Marouf Sofian, Andrew Bond, and
          <span class="more-authors" title="click to view 1 more author" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '1 more author' ? 'Gözde Şahin' : '1 more author';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">1 more author</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics</em>, Nov 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/2309.11346.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Grammatical Error Detection and Correction (GEC) tools have proven useful for native speakers and second language learners. Developing such tools requires a large amount of parallel, annotated data, which is unavailable for most languages. Synthetic data generation is a common practice to overcome the scarcity of such data. However, it is not straightforward for morphologically rich languages like Turkish due to complex writing rules that require phonological, morphological, and syntactic information. In this work, we present a flexible and extensible synthetic data generation pipeline for Turkish covering more than 20 expert-curated grammar and spelling rules (a.k.a., writing rules) implemented through complex transformation functions. Using the pipeline, we derive 130,000 high-quality parallel sentences from professionally edited articles. Additionally, we create a more realistic test set by manually annotating a set of movie reviews. We implement three baselines formulating the task as i) neural machine translation, ii) sequence tagging, and iii) few-shot learning with prefix tuning, achieving strong results. Then we perform a zero-shot evaluation of our pretrained models on the coarse-grained “BOUN -de/-da” and fine-grained expert annotated dataset. Our results suggest that our corpus, GECTurk, is high-quality and allows knowledge transfer for the out-of-domain setting. To encourage further research on Turkish GEC, we release our dataset, baseline models, and synthetic data generation pipeline with https://anonymous.4open.science/r/tr-gec-17D6/.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kara-EtAl:2023:findings</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kara, Atakan and Marouf Sofian, Farrin and Bond, Andrew and Şahin, Gözde}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{GECTurk: Grammatical Error Correction and Detection Dataset for Turkish}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Nusa Dua, Bali}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{278--290}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.findings-ijcnlp.26}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#e6ba8d"><a href="https://aclanthology.org/venues/inlg/" rel="external nofollow noopener" target="_blank">INLG</a></abbr></div>

        <!-- Entry bib key -->
        <div id="mbl-subha23" class="col-sm-8">
        <!-- Title -->
        <div class="title">Metric-Based In-context Learning: A Case Study in Text Simplification</div>
        <!-- Author -->
        <div class="author">
        

        Subha Vadlamannati, and Gözde Gül Şahin</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the 16th International Natural Language Generation Conference</em>, Sep 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/2023.inlg-main.18.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In-context learning (ICL) for large language models has proven to be a powerful approach for many natural language processing tasks. However, determining the best method to select examples for ICL is nontrivial as the results can vary greatly depending on the quality, quantity, and order of examples used. In this paper, we conduct a case study on text simplification (TS) to investigate how to select the best and most robust examples for ICL. We propose Metric-Based in-context Learning (MBL) method that utilizes commonly used TS metrics such as SARI, compression ratio, and BERT-Precision for selection. Through an extensive set of experiments with various-sized GPT models on standard TS benchmarks such as TurkCorpus and ASSET, we show that examples selected by the top SARI scores perform the best on larger models such as GPT-175B, while the compression ratio generally performs better on smaller models such as GPT-13B and GPT-6.7B. Furthermore, we demonstrate that MBL is generally robust to example orderings and out-of-domain test sets, and outperforms strong baselines and state-of-the-art finetuned language models. Finally, we show that the behaviour of large GPT models can be implicitly controlled by the chosen metric. Our research provides a new framework for selecting examples in ICL, and demonstrates its effectiveness in text simplification tasks, breaking new ground for more accurate and efficient NLG systems.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mbl-subha23</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Vadlamannati, Subha and Şahin, Gözde Gül}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Metric-Based In-context Learning: {A} Case Study in Text Simplification}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 16th International Natural Language Generation Conference}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Prague, Czech Republic}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#ff3200"><a href="https://aclanthology.org/venues/eacl/" rel="external nofollow noopener" target="_blank">EACL</a></abbr></div>

        <!-- Entry bib key -->
        <div id="klie-etal-2023-lessons" class="col-sm-8">
        <!-- Title -->
        <div class="title">Lessons Learned from a Citizen Science Project for Natural Language Processing</div>
        <!-- Author -->
        <div class="author">
        

        Jan-Christoph Klie, Ji-Ung Lee, Kevin Stowe, and
          <span class="more-authors" title="click to view 6 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '6 more authors' ? 'Gözde Gül Şahin, Nafise Sadat Moosavi, Luke Bates, Dominic Petrak, Richard Eckart De Castilho, Iryna Gurevych' : '6 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">6 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</em>, May 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/2023.eacl-main.261.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Many Natural Language Processing (NLP) systems use annotated corpora for training and evaluation. However, labeled data is often costly to obtain and scaling annotation projects is difficult, which is why annotation tasks are often outsourced to paid crowdworkers. Citizen Science is an alternative to crowdsourcing that is relatively unexplored in the context of NLP. To investigate whether and how well Citizen Science can be applied in this setting, we conduct an exploratory study into engaging different groups of volunteers in Citizen Science for NLP by re-annotating parts of a pre-existing crowdsourced dataset. Our results show that this can yield high-quality annotations and at- tract motivated volunteers, but also requires considering factors such as scalability, participation over time, and legal and ethical issues. We summarize lessons learned in the form of guidelines and provide our code and data to aid future work on Citizen Science.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">klie-etal-2023-lessons</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Lessons Learned from a Citizen Science Project for Natural Language Processing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Klie, Jan-Christoph and Lee, Ji-Ung and Stowe, Kevin and Şahin, Gözde Gül and Moosavi, Nafise Sadat and Bates, Luke and Petrak, Dominic and Eckart De Castilho, Richard and Gurevych, Iryna}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Dubrovnik, Croatia}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.eacl-main.261}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3594--3608}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#ff3200"><a href="https://aclanthology.org/venues/eacl/" rel="external nofollow noopener" target="_blank">EACL</a></abbr></div>

        <!-- Entry bib key -->
        <div id="DBLP:conf/eacl/PuertoSG23" class="col-sm-8">
        <!-- Title -->
        <div class="title">MetaQA: Combining Expert Agents for Multi-Skill Question Answering</div>
        <!-- Author -->
        <div class="author">
        

        Haritz Puerto, Gözde Gül Sahin, and Iryna Gurevych</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the 17th Conference of the European Chapter of the
                  Association for Computational Linguistics, EACL 2023, Dubrovnik,
                  Croatia, May 2-6, 2023</em>, May 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/2023.eacl-main.259.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The recent explosion of question answering (QA) datasets and models has increased the interest in the generalization of models across multiple domains and formats by either training on multiple datasets or by combining multiple models. Despite the promising results of multi-dataset models, some domains or QA formats may require specific architectures, and thus the adaptability of these models might be limited. In addition, current approaches for combining models disregard cues such as question-answer compatibility. In this work, we propose to combine expert agents with a novel, flexible, and training-efficient architecture that considers questions, answer predictions, and answer-prediction confidence scores to select the best answer among a list of answer candidates. Through quantitative and qualitative experiments we show that our model i) creates a collaboration between agents that outperforms previous multi-agent and multi-dataset approaches in both in-domain and out-of-domain scenarios, ii) is highly data-efficient to train, and iii) can be adapted to any QA format. We release our code and a dataset of answer predictions from expert agents for 16 QA datasets to foster future developments of multi-agent systems on https://github.com/UKPLab/MetaQA. </p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:conf/eacl/PuertoSG23</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Puerto, Haritz and Sahin, G{\"{o}}zde G{\"{u}}l and Gurevych, Iryna}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Vlachos, Andreas and Augenstein, Isabelle}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MetaQA: Combining Expert Agents for Multi-Skill Question Answering}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 17th Conference of the European Chapter of the
                    Association for Computational Linguistics, {EACL} 2023, Dubrovnik,
                    Croatia, May 2-6, 2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3548--3562}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.eacl-main.259}</span><span class="p">,</span>
  <span class="na">timestamp</span> <span class="p">=</span> <span class="s">{Thu, 11 May 2023 17:08:21 +0200}</span><span class="p">,</span>
  <span class="na">biburl</span> <span class="p">=</span> <span class="s">{https://dblp.org/rec/conf/eacl/PuertoSG23.bib}</span><span class="p">,</span>
  <span class="na">bibsource</span> <span class="p">=</span> <span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>


</div>


          </article>

        </div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 GGLab  . Photos from <a href="https://www.freepik.com/" target="_blank" rel="external nofollow noopener">Freepik</a>.
Last updated: March 03, 2025.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
